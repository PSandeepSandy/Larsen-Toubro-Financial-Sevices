{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/rajat5ranjan/AV-LTFS-Data-Science-FinHack-ML-Hackathon/raw/2853f792147b4305cad1b40d75893dab112e6611/ltfs.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello Everyone !\n",
    "\n",
    "This kernel consists of my work for the **AV - LTFS Hackathon** where we were supposed to predict the loan defaulters in the first month of EMI payment.\n",
    "\n",
    "I have tried some feature engineering first up, followed by parameter tuning of CatBoost and then a 1-Layer Stacking of the different base models.\n",
    "\n",
    "Other than CatBosst, XGBoost,LightGBM,RF,NNs were also tried, but they were giving sub-optimal results.\n",
    "\n",
    "On hind sight, a bit more extensive feature engineering would have helped in boosting the score further up.\n",
    "\n",
    "This kernel gets a \n",
    "\n",
    "**CV Score - 0.6752**\n",
    "\n",
    "**Public LB Score - 0.6636**       ( Rank - 53rd / 1352 )\n",
    "\n",
    "**Private LB Score - 0.667127**    ( Rank - 47th / 1352 )\n",
    "\n",
    "(AUC-ROC Metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Basic Overview of the things done in the kernel before jumping into the coding part - \n",
    "**\n",
    "\n",
    "1. FEATURE ENGINEERING - \n",
    "      *   **Anomalous Branch** - Keeps track of the branches, from where, certain loans have been sanctioned and then the buy has been done at a showroom  which is far from that bank,possibly even in a **different state or city**. This is tracked by seeing the usual showrooms from where buys take place if a loan is sanctioned from a particular branch. Certain anomalies detected in this list have been tracked in this feature.\n",
    "      *   The super-messy Perform_CNS Score categorical data have been **re-binned** to give a cleaner idea of the CIBIL scores. There have been 2 new binnings made. One on the basis of some background knowledge about banking, and the way banks segregate the users and the second according to the data provided in the dataset.\n",
    "      *   The number of ID Proofs a person has submitted at the time of taking the loan - Assumption being, **the more number of IDs shown, the more the credibility of the borrower.**\n",
    "      *   The number of Primary and Secondary accounts a person already has defaulted, overall as well as over the last 6 months.\n",
    "      *   The borrower's age, his/her average account age,i.e, on an average how much time he/she takes to give back all the lent money.\n",
    "      *   Whether the borrower is a **\"Student\"** or a **\"Senior Citizen\"** from the age and the employment status.\n",
    "      *   Since the model was suppposed to predict who would be defaulting in the **FIRST MONTH** of taking loan, so, keeping track of which all users defaulted in the **first month only**, rather than who all defaulted over-all in the train set makes more sense as the model would then be able to recognize the trends and behaviour more easily.\n",
    "      \n",
    "2. MISSING DATA HANDLING -\n",
    "      *   The missing \"Employment\" were treated as **\"Unemployed\"** as of that moment.\n",
    "      *   The UNEMPLOYED borrowers were categorized into \"Students\" and \"Senior Citizens\" taking a hint from their ages.\n",
    "      \n",
    "3. STRATIFICATION - \n",
    "\n",
    "      *   Stratification done on the basis of **similarity between the train and test set**, rather than doing on the basis of the classes.\n",
    "    \n",
    "4. TRAINING -\n",
    "\n",
    "      *   **Five-Fold Cross Validation** was used and the predictions on the test set were taken over the model trained on each fold and were finally averaged over all the folds to get the final prediction over the test set.\n",
    "      *   Heavy **Parameter Tuning done on CatBoost Classifier**, LightGBM, Random Forest and XGBoost, with CatBoost out-performing the rest. Hence, finally CatBoost was used for submission.\n",
    "      *   **Stacking** was done, with 20 CatBoost models and a meta learner (Logistic Regression) was used.\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test_bqCt9Pv.csv', 'train.csv', 'sample_submission_24jSKY6.csv']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from catboost import CatBoostClassifier,Pool\n",
    "from sklearn.model_selection import train_test_split,cross_val_predict,StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from bayes_opt import BayesianOptimization\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder,StandardScaler,MinMaxScaler,RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "#print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**READING THE FILES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/train.csv')\n",
    "test = pd.read_csv('../input/test_bqCt9Pv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UniqueID</th>\n",
       "      <th>disbursed_amount</th>\n",
       "      <th>asset_cost</th>\n",
       "      <th>ltv</th>\n",
       "      <th>branch_id</th>\n",
       "      <th>supplier_id</th>\n",
       "      <th>manufacturer_id</th>\n",
       "      <th>Current_pincode_ID</th>\n",
       "      <th>Date.of.Birth</th>\n",
       "      <th>Employment.Type</th>\n",
       "      <th>DisbursalDate</th>\n",
       "      <th>State_ID</th>\n",
       "      <th>Employee_code_ID</th>\n",
       "      <th>MobileNo_Avl_Flag</th>\n",
       "      <th>Aadhar_flag</th>\n",
       "      <th>PAN_flag</th>\n",
       "      <th>VoterID_flag</th>\n",
       "      <th>Driving_flag</th>\n",
       "      <th>Passport_flag</th>\n",
       "      <th>PERFORM_CNS.SCORE</th>\n",
       "      <th>PERFORM_CNS.SCORE.DESCRIPTION</th>\n",
       "      <th>PRI.NO.OF.ACCTS</th>\n",
       "      <th>PRI.ACTIVE.ACCTS</th>\n",
       "      <th>PRI.OVERDUE.ACCTS</th>\n",
       "      <th>PRI.CURRENT.BALANCE</th>\n",
       "      <th>PRI.SANCTIONED.AMOUNT</th>\n",
       "      <th>PRI.DISBURSED.AMOUNT</th>\n",
       "      <th>SEC.NO.OF.ACCTS</th>\n",
       "      <th>SEC.ACTIVE.ACCTS</th>\n",
       "      <th>SEC.OVERDUE.ACCTS</th>\n",
       "      <th>SEC.CURRENT.BALANCE</th>\n",
       "      <th>SEC.SANCTIONED.AMOUNT</th>\n",
       "      <th>SEC.DISBURSED.AMOUNT</th>\n",
       "      <th>PRIMARY.INSTAL.AMT</th>\n",
       "      <th>SEC.INSTAL.AMT</th>\n",
       "      <th>NEW.ACCTS.IN.LAST.SIX.MONTHS</th>\n",
       "      <th>DELINQUENT.ACCTS.IN.LAST.SIX.MONTHS</th>\n",
       "      <th>AVERAGE.ACCT.AGE</th>\n",
       "      <th>CREDIT.HISTORY.LENGTH</th>\n",
       "      <th>NO.OF_INQUIRIES</th>\n",
       "      <th>loan_default</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>420825</td>\n",
       "      <td>50578</td>\n",
       "      <td>58400</td>\n",
       "      <td>89.55</td>\n",
       "      <td>67</td>\n",
       "      <td>22807</td>\n",
       "      <td>45</td>\n",
       "      <td>1441</td>\n",
       "      <td>01-01-84</td>\n",
       "      <td>Salaried</td>\n",
       "      <td>03-08-18</td>\n",
       "      <td>6</td>\n",
       "      <td>1998</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>No Bureau History Available</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0yrs 0mon</td>\n",
       "      <td>0yrs 0mon</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>537409</td>\n",
       "      <td>47145</td>\n",
       "      <td>65550</td>\n",
       "      <td>73.23</td>\n",
       "      <td>67</td>\n",
       "      <td>22807</td>\n",
       "      <td>45</td>\n",
       "      <td>1502</td>\n",
       "      <td>31-07-85</td>\n",
       "      <td>Self employed</td>\n",
       "      <td>26-09-18</td>\n",
       "      <td>6</td>\n",
       "      <td>1998</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>598</td>\n",
       "      <td>I-Medium Risk</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>27600</td>\n",
       "      <td>50200</td>\n",
       "      <td>50200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1991</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1yrs 11mon</td>\n",
       "      <td>1yrs 11mon</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>417566</td>\n",
       "      <td>53278</td>\n",
       "      <td>61360</td>\n",
       "      <td>89.63</td>\n",
       "      <td>67</td>\n",
       "      <td>22807</td>\n",
       "      <td>45</td>\n",
       "      <td>1497</td>\n",
       "      <td>24-08-85</td>\n",
       "      <td>Self employed</td>\n",
       "      <td>01-08-18</td>\n",
       "      <td>6</td>\n",
       "      <td>1998</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>No Bureau History Available</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0yrs 0mon</td>\n",
       "      <td>0yrs 0mon</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>624493</td>\n",
       "      <td>57513</td>\n",
       "      <td>66113</td>\n",
       "      <td>88.48</td>\n",
       "      <td>67</td>\n",
       "      <td>22807</td>\n",
       "      <td>45</td>\n",
       "      <td>1501</td>\n",
       "      <td>30-12-93</td>\n",
       "      <td>Self employed</td>\n",
       "      <td>26-10-18</td>\n",
       "      <td>6</td>\n",
       "      <td>1998</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>305</td>\n",
       "      <td>L-Very High Risk</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0yrs 8mon</td>\n",
       "      <td>1yrs 3mon</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>539055</td>\n",
       "      <td>52378</td>\n",
       "      <td>60300</td>\n",
       "      <td>88.39</td>\n",
       "      <td>67</td>\n",
       "      <td>22807</td>\n",
       "      <td>45</td>\n",
       "      <td>1495</td>\n",
       "      <td>09-12-77</td>\n",
       "      <td>Self employed</td>\n",
       "      <td>26-09-18</td>\n",
       "      <td>6</td>\n",
       "      <td>1998</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>No Bureau History Available</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0yrs 0mon</td>\n",
       "      <td>0yrs 0mon</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UniqueID  disbursed_amount      ...       NO.OF_INQUIRIES  loan_default\n",
       "0    420825             50578      ...                     0             0\n",
       "1    537409             47145      ...                     0             1\n",
       "2    417566             53278      ...                     0             0\n",
       "3    624493             57513      ...                     1             1\n",
       "4    539055             52378      ...                     1             1\n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets have a look at the data\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    182543\n",
       "1     50611\n",
       "Name: loan_default, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets see how skewed the data is wrt the number of data points belonging to each class.\n",
    "\n",
    "train['loan_default'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FEATURE ENGINEERING - MAKING UP AS MANY NEW INNOVATIVE FEATURES AS I COULD COME UP WITH**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logic behind this feature is that, if a person takes a loan from a particular branch, in normal cases, we would expect him to buy the vehicle from a showroom/retailer which is located in the same city ( or the same state in worst case scenario ). \n",
    "\n",
    "So, my assumption was that every branch serves to customers who then go to one of the showrooms of a disjoint set, i.e, ideally, there should be a set of showrooms from where if a customer is buying a vehicle, then he must be getting it funded from a particular branch.\n",
    "\n",
    "Though this seemed to me to be somewhat logical assumption, it *didnt* really turn out to be that good a differentiator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "branchList = train['branch_id'].unique()\n",
    "branchSupId = train.groupby('branch_id')['supplier_id'].unique()\n",
    "\n",
    "branchSupIdList = []\n",
    "anomalousBranch = []\n",
    "\n",
    "for bra in range(len(branchList)):\n",
    "    branchId = branchList[bra]\n",
    "    branchSupIdList.append(branchSupId[branchId])\n",
    "\n",
    "for i in range(len(branchSupIdList)):\n",
    "  for j in range(len(branchSupIdList)):\n",
    "    if(i != j):\n",
    "      #print(len(list(set(branchSupIdList[i]).intersection(set(branchSupIdList[j])))))\n",
    "      if ((len(list(set(branchSupIdList[i]).intersection(set(branchSupIdList[j]))))) != 0):  \n",
    "        if (len(list(set(branchSupIdList[i]).intersection(set(branchSupIdList[j]))))) >= 3:  \n",
    "          #Both branches in the same locality.\n",
    "          continue\n",
    "        else:\n",
    "          anomalousBranch.append(branchList[i])\n",
    "      else:\n",
    "        #Disjoint Branches\n",
    "        continue\n",
    "    else:\n",
    "      continue  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isBranchAnomalous(x):\n",
    "  if (x in anomalousBranch):\n",
    "    return 1\n",
    "  else:\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CIBIL features are made from a bit of background knowledge. This is the usual score used by financial institutions in order to decide whether to lend money to a person or not.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The 'PERFORM_CNS.SCORE.DESCRIPTION' column has a lot of bins and there are many different kinds of bins which essentially represent more or less the same set of people/distribution of customers. \n",
    "\n",
    "**Ex - Different types of \"High Risk\", \"Low Risk\" etc.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CIBIL_norm(x):\n",
    "    a=''\n",
    "    if((x=='A-Very Low Risk') or (x=='B-Very Low Risk') or (x=='C-Very Low Risk') or (x=='D-Very Low Risk')):\n",
    "        a = 'Very Low Risk'\n",
    "    elif((x=='M-Very High Risk')):\n",
    "        a = 'Very Very High Risk'\n",
    "    elif((x=='L-Very High Risk')):\n",
    "        a='Very High Risk'\n",
    "    elif((x=='E-Low Risk') or (x=='F-Low Risk') or (x=='G-Low Risk')):\n",
    "        a = 'Low Risk'\n",
    "    elif((x=='H-Medium Risk') or (x=='I-Medium Risk')):\n",
    "        a = 'Medium Risk'\n",
    "    elif((x=='J-High Risk') or (x=='K-High Risk')):\n",
    "        a = 'High Risk'\n",
    "    elif((x=='Not Scored: No Activity seen on the customer (Inactive)') or (x=='Not Scored: No Updates available in last 36 months')):\n",
    "        a = 'Inactive'\n",
    "    elif((x=='Not Scored: Only a Guarantor')):\n",
    "        a='Guarantor'\n",
    "    elif((x=='Not Scored: More than 50 active Accounts found')):\n",
    "        a='SuperActive'\n",
    "    else:\n",
    "        a='Others'\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CIBIL_other(x):\n",
    "    a=''\n",
    "    if((x=='A-Very Low Risk') or (x=='B-Very Low Risk') or (x=='C-Very Low Risk') or (x=='D-Very Low Risk')):\n",
    "        a = 'Very Low Risk'\n",
    "    elif((x=='M-Very High Risk')):\n",
    "        a = 'Very Very High Risk'\n",
    "    elif((x=='L-Very High Risk')):\n",
    "        a='Very High Risk'\n",
    "    elif((x=='E-Low Risk') or (x=='F-Low Risk') or (x=='G-Low Risk')):\n",
    "        a = 'Low Risk'\n",
    "    elif((x=='H-Medium Risk') or (x=='I-Medium Risk')):\n",
    "        a = 'Medium Risk'\n",
    "    elif((x=='J-High Risk') or (x=='K-High Risk')):\n",
    "        a = 'High Risk'\n",
    "    elif((x=='Not Scored: No Activity seen on the customer (Inactive)') or (x=='Not Scored: No Updates available in last 36 months')):\n",
    "        a = 'Inactive'\n",
    "    elif((x=='Not Scored: Only a Guarantor')):\n",
    "        a='Guarantor'\n",
    "    elif((x=='Not Scored: More than 50 active Accounts found')):\n",
    "        a='SuperActive'\n",
    "    elif((x=='No Bureau History Available') or (x=='Not Scored: Sufficient History Not Available') or (x=='Not Scored: Not Enough Info available on the customer')):  \n",
    "        a='NoHistory'\n",
    "    else:\n",
    "        a='Others'\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CIBIL_trend(x):\n",
    "    a=''\n",
    "    if(x==300):\n",
    "        a='Very Poor'\n",
    "    elif((x>300) and (x<=550)):\n",
    "        a='Poor'\n",
    "    elif((x>550) and (x<=650)):\n",
    "        a='Fair'\n",
    "    elif((x>650) and (x<=750)):\n",
    "        a='Good'\n",
    "    elif((x>750) and (x<=900)):\n",
    "        a='Excellent'\n",
    "    else:\n",
    "        a='Others'\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of ID proofs submitted by a person while taking a loan.  - > Thought behind this is, **More the number of ID proofs a person submits, more is the chance of that person being a genuine person** and not someone who is intentionally going to default in EMI Payments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NumIds(x):\n",
    "    a=''\n",
    "    if(x==1):\n",
    "        a = 'One'\n",
    "    elif(x==2):\n",
    "        a='Two'\n",
    "    elif(x==3):\n",
    "        a='Three'\n",
    "    else:\n",
    "        a='Four'\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Age Calculation from the DOB column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcAge(x):\n",
    "    year = int(x.split('-')[2])\n",
    "    if(year<=19):\n",
    "        age = 20-year\n",
    "    else:\n",
    "        age = 100 + (20-year)\n",
    "    return age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remarks on the basis of the number of Primary and Secondary Defaulted accounts of that person.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrimaDefault(x):\n",
    "    a=''\n",
    "    if(x==-1):\n",
    "        a='First'\n",
    "    elif(x==0):\n",
    "        a='Great'\n",
    "    elif(x<=0.2):\n",
    "        a='Normal'\n",
    "    elif(x<=0.4):\n",
    "        a='Bothersome'\n",
    "    elif(x<=0.6):\n",
    "        a='Trouble'\n",
    "    elif(x<=0.8):\n",
    "        a='Danger'\n",
    "    else:\n",
    "        a='High Alert'\n",
    "    return a\n",
    "\n",
    "def SecDefault(x):\n",
    "    a=''\n",
    "    if(x==-1):\n",
    "        a='First'\n",
    "    elif(x==0):\n",
    "        a='Great'\n",
    "    elif(x<=0.2):\n",
    "        a='Normal'\n",
    "    elif(x<=0.4):\n",
    "        a='Bothersome'\n",
    "    elif(x<=0.6):\n",
    "        a='Trouble'\n",
    "    elif(x<=0.8):\n",
    "        a='Danger'\n",
    "    else:\n",
    "        a='High Alert'\n",
    "    return a\n",
    "\n",
    "def TotDefault(x):\n",
    "    a=''\n",
    "    if(x==-1):\n",
    "        a='First'\n",
    "    elif(x==0):\n",
    "        a='Great'\n",
    "    elif(x<=0.2):\n",
    "        a='Normal'\n",
    "    elif(x<=0.4):\n",
    "        a='Bothersome'\n",
    "    elif(x<=0.6):\n",
    "        a='Trouble'\n",
    "    elif(x<=0.8):\n",
    "        a='Danger'\n",
    "    else:\n",
    "        a='High Alert'\n",
    "    return a\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keeping track of the number of primary default accounts in the last 6 months and assigning a remark to that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrimaDefaultLastSix(x):\n",
    "    a=''\n",
    "    if(x==0):\n",
    "        a='Great'\n",
    "    elif(x==1):\n",
    "        a='Normal'\n",
    "    elif(x==2):\n",
    "        a='Bothersome'\n",
    "    elif(x>=3):\n",
    "        a='Trouble'\n",
    "    else:\n",
    "        a='Others'\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a new feature based on the Number of outstanding Balance accounts the customer has. \n",
    "\n",
    "The idea behind this being, **more the number of accounts a customer has with outstanding balance**, **the less reliable** he would be expected to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CurrOutstandingBal(x):\n",
    "    a=''\n",
    "    if (x==0):\n",
    "        a='Good'\n",
    "    elif((x<0) and (x!=-1)):\n",
    "        a='Very Good'\n",
    "    elif(x>0 and x<=1):\n",
    "        a='Both'\n",
    "    elif(x>1):\n",
    "        a='Problematic'\n",
    "    else:\n",
    "        a='Other'\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AvgAcctAge(x):\n",
    "    year = int(x.split(\" \")[0].split(\"y\")[0])\n",
    "    month = int(x.split(\" \")[1].split(\"m\")[0])\n",
    "    time_int = (12*year) + month\n",
    "    return time_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneMonth(x):\n",
    "    if(int(x.split('-')[1])==10):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature representing the number of identity cards given by the customer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "oneHot = train[['Aadhar_flag','PAN_flag','VoterID_flag','Driving_flag','Passport_flag']]\n",
    "oneHot['sum'] = oneHot['Aadhar_flag'] + oneHot['PAN_flag'] + oneHot['VoterID_flag'] + oneHot['Driving_flag'] + oneHot['Passport_flag'] \n",
    "train['NumIDs'] = oneHot['sum']\n",
    "\n",
    "oneHotTest = test[['Aadhar_flag','PAN_flag','VoterID_flag','Driving_flag','Passport_flag']]\n",
    "oneHotTest['sum'] = oneHotTest['Aadhar_flag'] + oneHotTest['PAN_flag'] + oneHotTest['VoterID_flag'] + oneHotTest['Driving_flag'] + oneHotTest['Passport_flag'] \n",
    "test['NumIDs'] = oneHotTest['sum']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Applying the functions defined above, and thus, the final steep in actually implementing the features planned above.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certain Scores on CNS Score like 11,14,15,16,17,18 were all made 0 as they all were corresponding to cases with less/no history of the borrower being available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simply Calling the functions defined above for feature engineering.\n",
    "\n",
    "#Train set dataframe manipulation\n",
    "\n",
    "train['NumIDsCnt'] = train['NumIDs'].apply(NumIds)\n",
    "train['IDsCount'] = np.where(train['NumIDs']>1,1,0)\n",
    "\n",
    "train['Age']=train['Date.of.Birth'].apply(calcAge)\n",
    "\n",
    "train['isStudent'] = np.where(train['Age']<=25,1,0)\n",
    "train['isSenior'] = np.where(train['Age']>=60,1,0)\n",
    "\n",
    "train['Employment.Type'] = np.where(train['Employment.Type'].isnull(),'Unemployed',train['Employment.Type'])\n",
    "\n",
    "train['leftover'] = train['asset_cost'] - train['disbursed_amount']\n",
    "train['loanRatio'] = (train['disbursed_amount']/train['asset_cost'])*100\n",
    "\n",
    "train['CIBIL_Descr'] = train['PERFORM_CNS.SCORE.DESCRIPTION'].apply(CIBIL_norm)\n",
    "train['CIBIL_Other'] = train['PERFORM_CNS.SCORE.DESCRIPTION'].apply(CIBIL_other)\n",
    "train['CIBIL_Trend'] = train['PERFORM_CNS.SCORE'].apply(CIBIL_trend)\n",
    "\n",
    "train['PriOverduePercentage'] = np.where(train['PRI.NO.OF.ACCTS'] != 0,train['PRI.OVERDUE.ACCTS']/train['PRI.NO.OF.ACCTS'],-1) \n",
    "train['SecOverduePercentage'] = np.where(train['SEC.NO.OF.ACCTS'] != 0,train['SEC.OVERDUE.ACCTS']/train['SEC.NO.OF.ACCTS'],-1) \n",
    "\n",
    "train['PrimaDefaultRemark'] = train['PriOverduePercentage'].apply(PrimaDefault)\n",
    "train['SecoDefaultRemark'] = train['SecOverduePercentage'].apply(SecDefault)\n",
    "train['totalDefaultPercent'] = np.where((train['PRI.NO.OF.ACCTS'] + train['SEC.NO.OF.ACCTS']) != 0,(train['PRI.OVERDUE.ACCTS'] + train['SEC.OVERDUE.ACCTS'])/(train['PRI.NO.OF.ACCTS'] + train['SEC.NO.OF.ACCTS']),-1)    \n",
    "train['TotaDefaultRemark'] = train['totalDefaultPercent'].apply(TotDefault) \n",
    "\n",
    "train['AcctsLastSixRemarks'] = train['DELINQUENT.ACCTS.IN.LAST.SIX.MONTHS'].apply(PrimaDefaultLastSix)\n",
    "\n",
    "train['PRIcritRatio'] = np.where(train['PRI.DISBURSED.AMOUNT'] != 0,train['PRI.CURRENT.BALANCE']/train['PRI.DISBURSED.AMOUNT'],-1)\n",
    "train['SECcritRatio'] = np.where(train['SEC.DISBURSED.AMOUNT'] != 0,train['SEC.CURRENT.BALANCE']/train['SEC.DISBURSED.AMOUNT'],-1)\n",
    "\n",
    "train['TOT.DISBURSED.AMOUNT'] = train['PRI.DISBURSED.AMOUNT'] + train['SEC.DISBURSED.AMOUNT']\n",
    "train['TOT.CURRENT.BALANCE'] = train['PRI.CURRENT.BALANCE'] + train['SEC.CURRENT.BALANCE']\n",
    "train['TOTcritRatio'] = np.where(train['TOT.DISBURSED.AMOUNT'] != 0,train['TOT.CURRENT.BALANCE']/train['TOT.DISBURSED.AMOUNT'],-1)\n",
    "\n",
    "train['PriRatioRemark'] = train['PRIcritRatio'].apply(CurrOutstandingBal)\n",
    "train['SecRatioRemark'] = train['SECcritRatio'].apply(CurrOutstandingBal)\n",
    "train['TotRatioRemark'] = train['TOTcritRatio'].apply(CurrOutstandingBal)\n",
    "\n",
    "train[\"AvgAcctAge\"] = train['AVERAGE.ACCT.AGE'].apply(AvgAcctAge)\n",
    "train['CredAcctAge'] = train['CREDIT.HISTORY.LENGTH'].apply(AvgAcctAge)\n",
    "\n",
    "train['OneMonthDef'] = train['DisbursalDate'].apply(oneMonth)\n",
    "\n",
    "train['PERFORM_CNS.SCORE'] = np.where(train['PERFORM_CNS.SCORE']==11,0,train['PERFORM_CNS.SCORE'])\n",
    "train['PERFORM_CNS.SCORE'] = np.where(train['PERFORM_CNS.SCORE']==14,0,train['PERFORM_CNS.SCORE'])\n",
    "train['PERFORM_CNS.SCORE'] = np.where(train['PERFORM_CNS.SCORE']==15,0,train['PERFORM_CNS.SCORE'])\n",
    "train['PERFORM_CNS.SCORE'] = np.where(train['PERFORM_CNS.SCORE']==16,0,train['PERFORM_CNS.SCORE'])\n",
    "train['PERFORM_CNS.SCORE'] = np.where(train['PERFORM_CNS.SCORE']==17,0,train['PERFORM_CNS.SCORE'])\n",
    "train['PERFORM_CNS.SCORE'] = np.where(train['PERFORM_CNS.SCORE']==18,0,train['PERFORM_CNS.SCORE'])\n",
    "\n",
    "train['isBranchAnomalous'] = train['branch_id'].apply(isBranchAnomalous)\n",
    "\n",
    "#Test Set dataframe manipulation\n",
    "\n",
    "\n",
    "test['isBranchAnomalous'] = test['branch_id'].apply(isBranchAnomalous)\n",
    "\n",
    "test['NumIDsCnt'] = test['NumIDs'].apply(NumIds)\n",
    "test['IDsCount'] = np.where(test['NumIDs']>1,1,0)\n",
    "\n",
    "test['Age']=test['Date.of.Birth'].apply(calcAge)\n",
    "\n",
    "test['isStudent'] = np.where(test['Age']<=25,1,0)\n",
    "test['isSenior'] = np.where(test['Age']>=60,1,0)\n",
    "\n",
    "test['Employment.Type'] = np.where(test['Employment.Type'].isnull(),'Unemployed',test['Employment.Type'])\n",
    "\n",
    "test['leftover'] = test['asset_cost'] - test['disbursed_amount']\n",
    "test['loanRatio'] = (test['disbursed_amount']/test['asset_cost'])*100\n",
    "\n",
    "test['CIBIL_Descr'] = test['PERFORM_CNS.SCORE.DESCRIPTION'].apply(CIBIL_norm)\n",
    "test['CIBIL_Other'] = test['PERFORM_CNS.SCORE.DESCRIPTION'].apply(CIBIL_other)\n",
    "test['CIBIL_Trend'] = test['PERFORM_CNS.SCORE'].apply(CIBIL_trend)\n",
    "\n",
    "test['PriOverduePercentage'] = np.where(test['PRI.NO.OF.ACCTS'] != 0,test['PRI.OVERDUE.ACCTS']/test['PRI.NO.OF.ACCTS'],-1) \n",
    "test['SecOverduePercentage'] = np.where(test['SEC.NO.OF.ACCTS'] != 0,test['SEC.OVERDUE.ACCTS']/test['SEC.NO.OF.ACCTS'],-1) \n",
    "\n",
    "test['PrimaDefaultRemark'] = test['PriOverduePercentage'].apply(PrimaDefault)\n",
    "test['SecoDefaultRemark'] = test['SecOverduePercentage'].apply(SecDefault)\n",
    "test['totalDefaultPercent'] = np.where((test['PRI.NO.OF.ACCTS'] + test['SEC.NO.OF.ACCTS']) != 0,(test['PRI.OVERDUE.ACCTS'] + test['SEC.OVERDUE.ACCTS'])/(test['PRI.NO.OF.ACCTS'] + test['SEC.NO.OF.ACCTS']),-1)    \n",
    "test['TotaDefaultRemark'] = test['totalDefaultPercent'].apply(TotDefault) \n",
    "\n",
    "test['AcctsLastSixRemarks'] = test['DELINQUENT.ACCTS.IN.LAST.SIX.MONTHS'].apply(PrimaDefaultLastSix)\n",
    "\n",
    "test['PRIcritRatio'] = np.where(test['PRI.DISBURSED.AMOUNT'] != 0,test['PRI.CURRENT.BALANCE']/test['PRI.DISBURSED.AMOUNT'],-1)\n",
    "test['SECcritRatio'] = np.where(test['SEC.DISBURSED.AMOUNT'] != 0,test['SEC.CURRENT.BALANCE']/test['SEC.DISBURSED.AMOUNT'],-1)\n",
    "\n",
    "test['TOT.DISBURSED.AMOUNT'] = test['PRI.DISBURSED.AMOUNT'] + test['SEC.DISBURSED.AMOUNT']\n",
    "test['TOT.CURRENT.BALANCE'] = test['PRI.CURRENT.BALANCE'] + test['SEC.CURRENT.BALANCE']\n",
    "test['TOTcritRatio'] = np.where(test['TOT.DISBURSED.AMOUNT'] != 0,test['TOT.CURRENT.BALANCE']/test['TOT.DISBURSED.AMOUNT'],-1)\n",
    "\n",
    "test['PriRatioRemark'] = test['PRIcritRatio'].apply(CurrOutstandingBal)\n",
    "test['SecRatioRemark'] = test['SECcritRatio'].apply(CurrOutstandingBal)\n",
    "test['TotRatioRemark'] = test['TOTcritRatio'].apply(CurrOutstandingBal)\n",
    "\n",
    "test[\"AvgAcctAge\"] = test['AVERAGE.ACCT.AGE'].apply(AvgAcctAge)\n",
    "test['CredAcctAge'] = test['CREDIT.HISTORY.LENGTH'].apply(AvgAcctAge)\n",
    "\n",
    "test['OneMonthDef'] = test['DisbursalDate'].apply(oneMonth)\n",
    "\n",
    "test['PERFORM_CNS.SCORE'] = np.where(test['PERFORM_CNS.SCORE']==11,0,test['PERFORM_CNS.SCORE'])\n",
    "test['PERFORM_CNS.SCORE'] = np.where(test['PERFORM_CNS.SCORE']==14,0,test['PERFORM_CNS.SCORE'])\n",
    "test['PERFORM_CNS.SCORE'] = np.where(test['PERFORM_CNS.SCORE']==15,0,test['PERFORM_CNS.SCORE'])\n",
    "test['PERFORM_CNS.SCORE'] = np.where(test['PERFORM_CNS.SCORE']==16,0,test['PERFORM_CNS.SCORE'])\n",
    "test['PERFORM_CNS.SCORE'] = np.where(test['PERFORM_CNS.SCORE']==17,0,test['PERFORM_CNS.SCORE'])\n",
    "test['PERFORM_CNS.SCORE'] = np.where(test['PERFORM_CNS.SCORE']==18,0,test['PERFORM_CNS.SCORE'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(columns=['Date.of.Birth','DisbursalDate','UniqueID'],inplace=True)\n",
    "test.drop(columns=['Date.of.Birth','DisbursalDate','UniqueID'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Making a function for easier Training and Cross-Validation - Using 5 fold stratified cross-validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoreOfModel(clf,X,y,flag,shuffleBool=False,nFolds=5):\n",
    "    score = 0\n",
    "    finalPreds = np.zeros(112392)\n",
    "    trainPreds = np.zeros(233154)\n",
    "    folds = StratifiedKFold(n_splits=nFolds, shuffle=shuffleBool, random_state=42)\n",
    "    #train_pred = cross_val_predict(clf, X, y, cv=12,method='predict_proba')\n",
    "    for fold_, (trn_idx, val_idx) in tqdm(enumerate(folds.split(X,stratCol))):\n",
    "        X_train,X_val = X.loc[trn_idx,:],X.loc[val_idx,:]\n",
    "        y_train,y_val = y[trn_idx],y[val_idx]\n",
    "        clf.fit(X_train,y_train)\n",
    "        yPreds = clf.predict_proba(X_val)\n",
    "        yPredsTweaked = yPreds[:,1]\n",
    "        trainPreds[val_idx] = yPredsTweaked\n",
    "        score += roc_auc_score(y_val,yPredsTweaked)\n",
    "        p = clf.predict_proba(x_test)\n",
    "        #Adding the probabilities of belonging to the class \"1\".\n",
    "        for k in range(len(p)):\n",
    "            finalPreds[k] += p[k][1]\n",
    "        print(\"**********\"+ str(score/(1+fold_)) + \"******************Iteration \"+str(fold_)+\" Done****************\")    \n",
    "    return str(score/nFolds),(trainPreds),(finalPreds/nFolds)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelEnc = ['AVERAGE.ACCT.AGE','CREDIT.HISTORY.LENGTH',\n",
    "'branch_id', 'supplier_id','manufacturer_id','State_ID','PERFORM_CNS.SCORE.DESCRIPTION','Current_pincode_ID',\n",
    "                                          'isStudent','isSenior','Employment.Type',\n",
    "                                                  'CIBIL_Trend','AcctsLastSixRemarks','OneMonthDef',\n",
    "                                                  'NumIDsCnt','CIBIL_Descr','CIBIL_Other',\n",
    "                                                  'PrimaDefaultRemark','SecoDefaultRemark','TotaDefaultRemark',\n",
    "                                                  'PriRatioRemark','SecRatioRemark','TotRatioRemark',  'Employee_code_ID'\n",
    "                                                    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(columns=['loan_default'])\n",
    "y = train['loan_default']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using \"similarity between the train and test columns\" as the stratification. **\n",
    "\n",
    "Instead of using the class labels as stratification,using the similarity between the train and test set as a parameter for stratification tends to give a better model, considering that the model gets an idea about how similar/dissimilar the data points in train and test set are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "data = pd.concat([X, test], axis = 0)\n",
    "\n",
    "X_newLGB = data.copy()\n",
    "#test_newLGB = test.copy()\n",
    "for col in labelEnc:\n",
    "  le = LabelEncoder()\n",
    "  data[col] = le.fit_transform(data[col])\n",
    "  #X_newLGB[col] = le.fit_transform(data[col])\n",
    "  #test_newLGB[col] = le.transform(test[col])\n",
    "\n",
    "data['is_test'] = np.zeros(345546)\n",
    "\n",
    "#(data.iloc[:233154,:])['is_test'] = 0\n",
    "data.iloc[233154:,-1] = 1\n",
    "\n",
    "train_examples = train.shape[0]\n",
    "\n",
    "data_x = data.drop('is_test', axis=1)\n",
    "data_y = data['is_test']\n",
    "\n",
    "is_test_probs = cross_val_predict(RandomForestClassifier(max_depth = 7,n_estimators=200), data_x, data_y, method='predict_proba')[:train_examples]\n",
    "\n",
    "is_test_Probs = is_test_probs[:,1]\n",
    "\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "data.iloc[:233154,-1] = rankdata(is_test_Probs)\n",
    "bins = np.histogram(data.iloc[:233154,-1])[1][:-1]\n",
    "#train['is_test_bins'] = np.digitize(X_newLGB['is_test'], bins)\n",
    "stratCol = np.digitize(data.iloc[:233154,-1], bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data.iloc[:233154,:]\n",
    "x_test = data.iloc[233154:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Categorical Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "catFeatures = ['AVERAGE.ACCT.AGE','CREDIT.HISTORY.LENGTH','branch_id', 'supplier_id','manufacturer_id','State_ID','PERFORM_CNS.SCORE.DESCRIPTION','Current_pincode_ID',\n",
    "                                          'isStudent','isSenior','Employment.Type',\n",
    "                                                  'CIBIL_Trend','AcctsLastSixRemarks','OneMonthDef',\n",
    "                                                  'NumIDsCnt','CIBIL_Descr','CIBIL_Other',\n",
    "                                                  'PrimaDefaultRemark','SecoDefaultRemark','TotaDefaultRemark',\n",
    "                                                  'PriRatioRemark','SecRatioRemark','TotRatioRemark',  'Employee_code_ID' \n",
    "                                         ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training different base models with different hyper-paramter settings for stacking.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the final submission, I have stacked all the 20 base models with the hyper-parameters which I have mentioned in the comments below as well. \n",
    "\n",
    "Due to the restriction on the maximum time a kernel can run for getting committed on Kaggle, I am unable to run a complete stack of 20 models here.\n",
    "\n",
    "Here I have trained only 3 models and stacked on those.\n",
    "\n",
    "Hence, this result can be expected to be a bit *sub-optimal* than the max scores which I have actually achieved in the hackathon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# catClf1 = CatBoostClassifier(learning_rate = 0.02147,iterations = 9997, l2_leaf_reg = 9985,scale_pos_weight = 3.662,eval_metric='AUC',\n",
    "#                             silent = True,cat_features=catFeatures)\n",
    "\n",
    "catClf2 = CatBoostClassifier(learning_rate = 0.03185,iterations = 2000, l2_leaf_reg = 999.6,scale_pos_weight = 1.915,eval_metric='AUC',\n",
    "                            silent = True,cat_features=catFeatures)\n",
    "\n",
    "catClf3 = CatBoostClassifier(learning_rate = 0.03998,iterations = 1497, l2_leaf_reg = 49.97,scale_pos_weight = 2.207,eval_metric='AUC',\n",
    "                            silent = True,cat_features=catFeatures)\n",
    "\n",
    "# catClf4 = CatBoostClassifier(learning_rate = 0.02838,iterations = 5174, l2_leaf_reg = 6311,scale_pos_weight = 3.926,eval_metric='AUC',\n",
    "#                             silent = True,cat_features=catFeatures)\n",
    "\n",
    "# catClf5 = CatBoostClassifier(learning_rate = 0.02373,iterations = 3174, l2_leaf_reg = 2739,scale_pos_weight = 2.228,eval_metric='AUC',\n",
    "#                             silent = True,cat_features=catFeatures)\n",
    "\n",
    "# catClf6 = CatBoostClassifier(learning_rate = 0.02,iterations = 5336, l2_leaf_reg = 7763,scale_pos_weight = 2.012,eval_metric='AUC',\n",
    "#                             silent = True,cat_features=catFeatures)\n",
    "\n",
    "# catClf7 = CatBoostClassifier(learning_rate = 0.03624,iterations = 5995, l2_leaf_reg = 9994,scale_pos_weight = 0.8615,eval_metric='AUC',\n",
    "#                             silent = True,cat_features=catFeatures)\n",
    "\n",
    "catClf8 = CatBoostClassifier(learning_rate = 0.03365,iterations = 2001, l2_leaf_reg = 9943,scale_pos_weight = 4.617,eval_metric='AUC',\n",
    "                            silent = True,cat_features=catFeatures)\n",
    "\n",
    "# catClf9 = CatBoostClassifier(learning_rate = 0.03132,iterations = 9985, l2_leaf_reg = 9988,scale_pos_weight = 0.6724,eval_metric='AUC',\n",
    "#                            silent = True,cat_features=catFeatures)\n",
    "\n",
    "#catClf10 = CatBoostClassifier(learning_rate = 0.0379,iterations = 2001, l2_leaf_reg = 5651,scale_pos_weight = 3.447,eval_metric='AUC',\n",
    "#                            silent = True,cat_features=catFeatures)\n",
    "\n",
    "#catClf11 = CatBoostClassifier(learning_rate = 0.02852,iterations = 2015, l2_leaf_reg = 2005,scale_pos_weight = 1.301,eval_metric='AUC',\n",
    "#                            silent = True,cat_features=catFeatures)\n",
    "\n",
    "# catClf12 = CatBoostClassifier(learning_rate = 0.02705,iterations = 6150, l2_leaf_reg = 9998,scale_pos_weight = 4.336,eval_metric='AUC',\n",
    "#                             silent = True,cat_features=catFeatures)\n",
    "\n",
    "# catClf13 = CatBoostClassifier(learning_rate = 0.02706,iterations = 9996, l2_leaf_reg = 6891,scale_pos_weight = 0.596,eval_metric='AUC',\n",
    "#                             silent = True,cat_features=catFeatures)\n",
    "\n",
    "# catClf14 = CatBoostClassifier(learning_rate = 0.0396,iterations = 5582, l2_leaf_reg = 2002,scale_pos_weight = 2.781,eval_metric='AUC',\n",
    "#                             silent = True,cat_features=catFeatures)\n",
    "\n",
    "# catClf15 = CatBoostClassifier(learning_rate = 0.03604,iterations = 9958, l2_leaf_reg = 10000,scale_pos_weight = 3.879,eval_metric='AUC',\n",
    "#                             silent = True,cat_features=catFeatures)\n",
    "\n",
    "# catClf16 = CatBoostClassifier(learning_rate = 0.02844,iterations = 7360, l2_leaf_reg = 6280,scale_pos_weight = 0.6643,eval_metric='AUC',\n",
    "#                             silent = True,cat_features=catFeatures)\n",
    "\n",
    "# catClf17 = CatBoostClassifier(learning_rate = 0.03624,iterations = 5995, l2_leaf_reg = 9994,scale_pos_weight = 0.8615,eval_metric='AUC',\n",
    "#                             silent = True,cat_features=catFeatures)\n",
    "\n",
    "# catClf18 = CatBoostClassifier(learning_rate = 0.03098,iterations = 2002, l2_leaf_reg = 6313,scale_pos_weight = 5.341,eval_metric='AUC',\n",
    "#                             silent = True,cat_features=catFeatures)\n",
    "\n",
    "# catClf19 = CatBoostClassifier(learning_rate = 0.02764,iterations = 7314, l2_leaf_reg = 7162,scale_pos_weight = 5.475,eval_metric='AUC',\n",
    "#                             silent = True,cat_features=catFeatures)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbdf833ea7944604a252c94faf3c5cfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********0.6707891258021255******************Iteration 0 Done****************\n",
      "**********0.6726077168447971******************Iteration 1 Done****************\n",
      "**********0.6747815393125444******************Iteration 2 Done****************\n",
      "**********0.672849107699964******************Iteration 3 Done****************\n",
      "**********0.671489010478794******************Iteration 4 Done****************\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe7a8f409b104f72825c651c6e70b576",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********0.670576090707276******************Iteration 0 Done****************\n",
      "**********0.6723666268762918******************Iteration 1 Done****************\n",
      "**********0.6743944644994464******************Iteration 2 Done****************\n",
      "**********0.6725074264828839******************Iteration 3 Done****************\n",
      "**********0.671190634112752******************Iteration 4 Done****************\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07825844255d48a8a1929d6a9c73a64d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********0.6701874883036151******************Iteration 0 Done****************\n",
      "**********0.6716761562409295******************Iteration 1 Done****************\n",
      "**********0.6737274957991222******************Iteration 2 Done****************\n",
      "**********0.6716625837014174******************Iteration 3 Done****************\n",
      "**********0.6702635494906773******************Iteration 4 Done****************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#scr_catClf1,trainPredsProbas1,catClfPreds1 = scoreOfModel(catClf1,x_train,y,3)\n",
    "scr_catClf2,trainPredsProbas2,catClfPreds2 = scoreOfModel(catClf2,x_train,y,3)\n",
    "scr_catClf3,trainPredsProbas3,catClfPreds3 = scoreOfModel(catClf3,x_train,y,3)\n",
    "#scr_catClf4,trainPredsProbas4,catClfPreds4 = scoreOfModel(catClf4,x_train,y,3)\n",
    "#scr_catClf5,trainPredsProbas5,catClfPreds5 = scoreOfModel(catClf5,x_train,y,3)\n",
    "#scr_catClf6,trainPredsProbas6,catClfPreds6 = scoreOfModel(catClf6,x_train,y,3)\n",
    "#scr_catClf7,trainPredsProbas7,catClfPreds7 = scoreOfModel(catClf7,x_train,y,3)\n",
    "scr_catClf8,trainPredsProbas8,catClfPreds8 = scoreOfModel(catClf8,x_train,y,3)\n",
    "#scr_catClf9,trainPredsProbas9,catClfPreds9 = scoreOfModel(catClf9,x_train,y,3)\n",
    "#scr_catClf10,trainPredsProbas10,catClfPreds10 = scoreOfModel(catClf10,x_train,y,3)\n",
    "#scr_catClf11,trainPredsProbas11,catClfPreds11 = scoreOfModel(catClf11,x_train,y,3)\n",
    "# scr_catClf12,trainPredsProbas12,catClfPreds12 = scoreOfModel(catClf12,x_train,y,3)\n",
    "# scr_catClf13,trainPredsProbas13,catClfPreds13 = scoreOfModel(catClf13,x_train,y,3)\n",
    "# scr_catClf14,trainPredsProbas14,catClfPreds14 = scoreOfModel(catClf14,x_train,y,3)\n",
    "# scr_catClf15,trainPredsProbas15,catClfPreds15 = scoreOfModel(catClf15,x_train,y,3)\n",
    "# scr_catClf16,trainPredsProbas16,catClfPreds16 = scoreOfModel(catClf16,x_train,y,3)\n",
    "# scr_catClf17,trainPredsProbas17,catClfPreds17 = scoreOfModel(catClf17,x_train,y,3)\n",
    "# scr_catClf18,trainPredsProbas18,catClfPreds18 = scoreOfModel(catClf18,x_train,y,3)\n",
    "# scr_catClf19,trainPredsProbas19,catClfPreds19 = scoreOfModel(catClf19,x_train,y,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stacking the base models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining a new Dataframe with the prediction values from our previous base models and then, we will use this DataFrame to train a meta-learner ( Logistic Regression in this kernel ) to get a boost in the prediction levels.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "stackedDF = pd.DataFrame({#'One' : trainPredsProbas1,\n",
    "                          'Two' : trainPredsProbas2,'Three' : trainPredsProbas3, \n",
    "                          # 'Four' : trainPredsProbas4, 'Five' : trainPredsProbas5, 'Six' : trainPredsProbas6,\n",
    "                          #'Seven':trainPredsProbas7,\n",
    "                           'Eight':trainPredsProbas8,\n",
    "                            #'Nine':trainPredsProbas9,\n",
    "                          #'Ten':trainPredsProbas10,'Eleven':trainPredsProbas11\n",
    "                            #,'Twelve':trainPredsProbas12,\n",
    "                          #'Thirteen':trainPredsProbas13,'Fourteen':trainPredsProbas14,\n",
    "                          #'Fifteen':trainPredsProbas15,'Sixteen':trainPredsProbas16,'Seventeen':trainPredsProbas17,\n",
    "                          #'Eighteen':trainPredsProbas18,'Nineteen':trainPredsProbas19\n",
    "                         })\n",
    "\n",
    "stackedTest = pd.DataFrame({#'One' : catClfPreds1,\n",
    "                            'Two' : catClfPreds2, 'Three' : catClfPreds3,\n",
    "                            #'Four' : catClfPreds4,'Five' : catClfPreds5, 'Six' : catClfPreds6,\n",
    "                           #'Seven':catClfPreds7,\n",
    "                           'Eight':catClfPreds8,\n",
    "                            #'Nine':catClfPreds9,\n",
    "                         # 'Ten':catClfPreds10,'Eleven':catClfPreds11\n",
    "                            #,'Twelve':catClfPreds12,\n",
    "                          #'Thirteen':catClfPreds13,'Fourteen':catClfPreds14,\n",
    "                          #'Fifteen':catClfPreds15,'Sixteen':catClfPreds16,'Seventeen':catClfPreds17,\n",
    "                          #'Eighteen':catClfPreds18,'Nineteen':catClfPreds19\n",
    "                         })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stacker = LogisticRegression(C = 0.003728,solver='liblinear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Stacker with **5 fold CV stratified**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b9bcfb6bbfd4943928fd51fda072a54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "LRprobas = np.zeros(112392)\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=False, random_state=42)\n",
    "for fold_, (trn_idx, val_idx) in tqdm(enumerate(folds.split(stackedDF,y))):\n",
    "    X_train,X_val = stackedDF.loc[trn_idx,:],stackedDF.loc[val_idx,:]\n",
    "    y_train,y_val = y[trn_idx],y[val_idx]\n",
    "    \n",
    "    Stacker.fit(X_train,y_train)\n",
    "    \n",
    "    #LRpreda = Stacker.predict_proba(X_val)\n",
    "    #LRtrainprobas[val_idx] = LRpreda[:,1]\n",
    "    #LRpredaT = LRpreda[:,1]\n",
    "    #LRscore = LRscore + roc_auc_score(y_val,LRpredaT)\n",
    "    LRpreds = Stacker.predict_proba(stackedTest)\n",
    "    LRprobas = LRprobas + LRpreds[:,1]\n",
    "\n",
    "LRprobas = LRprobas/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('../input/sample_submission_24jSKY6.csv')\n",
    "\n",
    "sub['loan_default'] = LRprobas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('Submission.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now that we have reached the end of the kernel, I am assuming you liked the kernel, since you didnt close it mid-way.**\n",
    "\n",
    "**If you did like it, please UPVOTE the kernel. That keeps me going !**\n",
    "\n",
    "**Any suggestions and criticism are welcome.**\n",
    "\n",
    "**Cheers !**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
